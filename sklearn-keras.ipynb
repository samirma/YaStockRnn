{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 503083,
     "status": "ok",
     "timestamp": 1568068274112,
     "user": {
      "displayName": "Samir Moreira Ant√¥nio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCsh3ldsSLN0VnFRYp341EzO_UyuRNKmLncUbMA3Q=s64",
      "userId": "14581632111443153742"
     },
     "user_tz": -60
    },
    "id": "fGVQczSlF-9o",
    "outputId": "e3a06798-f737-48d9-ec6e-d3784cec3115"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from data_generator import DataGenerator\n",
    "from state_util import StateUtil\n",
    "from tec_an import TecAn\n",
    "import numpy as np\n",
    "from data_util import *\n",
    "from sklearn_model_hyper import *\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scs\n",
    "import scikitplot as skplt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import InputLayer, BatchNormalization, GlobalMaxPool1D, Bidirectional, Dense, Flatten, Conv2D, LeakyReLU, Dropout, LSTM, GRU, Input\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from data_generator import DataGenerator\n",
    "from state_util import StateUtil\n",
    "from tec_an import TecAn\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "trainX_raw, trainY_raw = load_data(\"\", \"train\", path)\n",
    "valX_raw, valY_raw = load_data(\"\", \"Val\", path)\n",
    "\n",
    "#trainX_balanced, trainY_balanced = get_balanced_set(trainX_raw, trainY_raw)\n",
    "\n",
    "valX, valY = valX_raw, valY_raw\n",
    "\n",
    "features = trainX_raw.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Counter(trainY))\n",
    "\n",
    "\n",
    "X_over, y_over = get_balanced_set(trainX_raw, trainY_raw)\n",
    "\n",
    "print()\n",
    "print(\"{} {}\".format(Counter(y_over), Counter(trainY_raw)))\n",
    "\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(input_shape=(features,))\n",
    "normalizer.adapt(X_over)\n",
    "\n",
    "print(\"{} {} | {} {}\".format(X_over.shape, y_over.shape, valX.shape, valY.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "def get_balanced_set(x, y):\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    #print(Counter(y))\n",
    "    X_over, y_over = oversample.fit_resample(x, y)\n",
    "    #print(Counter(y_over))\n",
    "    return X_over, y_over\n",
    "\n",
    "\n",
    "metrics = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "#metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_full_model(\n",
    "                  features = 3, \n",
    "                  layers = 1, \n",
    "                  units = 5, \n",
    "                  optimizer='Adam',\n",
    "                  activation='relu',\n",
    "                  k_reg=regularizers.l2(0.01),\n",
    "                  init='glorot_uniform',\n",
    "                  lays_seqs = 3\n",
    "                 ):\n",
    "\n",
    "    set_seeds()\n",
    "    \n",
    "    #print(\"{} {}\".format(activation, optimizer))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #, input_shape=(seqs, features)\n",
    "    \n",
    "    model.add(normalizer)\n",
    "        \n",
    "    model.add(Dense(units, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation=activation))\n",
    "    \n",
    "\n",
    "    for lay in range(int(layers)):\n",
    "        model.add(Dense(units, \n",
    "                        kernel_initializer=init, \n",
    "                        kernel_regularizer=k_reg,\n",
    "                        activation=activation))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation='sigmoid')) \n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#model = get_rnn_model(**params)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get_full_model\n",
    "\n",
    "\n",
    "ea = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_precision\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = KerasClassifier(build_fn=get_full_model, \n",
    "                        callbacks=[ea],\n",
    "                        validation_data = (valX, valY),\n",
    "                        verbose=0\n",
    "                       )\n",
    "# grid search epochs, batch size and optimizer\n",
    "\n",
    "\n",
    "init = [\n",
    "        'glorot_uniform', \n",
    "        #, 'normal' \n",
    "        'he_uniform'\n",
    "        ]\n",
    "\n",
    "\n",
    "X_train, Y_train = X_over, y_over\n",
    "\n",
    "\n",
    "param_grid = dict(\n",
    "    init = init,\n",
    "    optimizer = [\n",
    "              \"Adadelta\",\n",
    "              \"Nadam\"\n",
    "              , \"Adam\"\n",
    "            ], \n",
    "    epochs=[100], \n",
    "    batch_size=[64],\n",
    "    layers = [\n",
    "        5, \n",
    "        15, \n",
    "        20],\n",
    "    units = [\n",
    "        16,\n",
    "        32,\n",
    "        64\n",
    "            ],\n",
    "    features = [features],\n",
    "    k_reg=[ \n",
    "        \"l2\", \n",
    "        \"l1\"\n",
    "    ],\n",
    "    activation = [\n",
    "        #'softmax', \n",
    "        #'softplus', \n",
    "        #'softsign', \n",
    "        'tanh', \n",
    "        'relu', \n",
    "        'sigmoid', \n",
    "        #'hard_sigmoid', \n",
    "        #'linear'\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "clf_grid = gridSearch(X_over, y_over, model, param_grid)\n",
    "\n",
    "#10181818181818181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid.best_params_\n",
    "\n",
    "param_test = clf_grid.best_params_\n",
    "\n",
    "del param_test['batch_size']\n",
    "del param_test['epochs']\n",
    "\n",
    "param_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#model = get_full_model(**clf_grid.best_params_)\n",
    "#model = get_full_model(**param_test)\n",
    "\n",
    "#x_norm = normalizer(X_over).numpy()\n",
    "\n",
    "#model.fit(\n",
    "#          X_over, y_over,\n",
    "#          epochs=100, \n",
    "#          verbose=True,\n",
    "#          batch_size=64,\n",
    "#          callbacks=[ea],\n",
    "#          validation_data = (valX, valY),\n",
    "#          )\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_norm = normalizer(valX).numpy()\n",
    "\n",
    "eval_data(model, valX, valY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'Keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "enviroment",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
