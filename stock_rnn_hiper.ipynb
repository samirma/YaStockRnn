{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WbTLRASrfYo8",
    "outputId": "4e0598ae-3da5-4b78-a0a8-5b0b29c3d055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-rc3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "from data_util import *\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalMaxPool1D, Bidirectional, Dense, Flatten, Conv2D, LeakyReLU, Dropout, LSTM, GRU, Input\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "%load_ext tensorboard\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "    seed_value= 0\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value)\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2vRFxrhCB946",
    "outputId": "042612cf-7ab5-4094-8ffa-1ddb40f227fd"
   },
   "outputs": [],
   "source": [
    "#Model \n",
    "\n",
    "def add_deep_layers(x, drop, units):\n",
    "    #x = BatchNormalization()(input_layer)\n",
    "    x = Dropout(drop)(x)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    #x = Dropout(drop)(x)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def add_attention(x):\n",
    "    attention = tf.keras.layers.Attention(use_scale=True)([x, x])\n",
    "    x = attention\n",
    "    return x\n",
    "\n",
    "def generate_model(seqs, features, dim, dropout, lays, lays_seq):\n",
    "    inputX = Input(shape=(seqs, features))\n",
    "    #x = Dense(dim)(inputX)\n",
    "    x = inputX\n",
    "    #x = add_attention(x)\n",
    "    x = LSTM(units=dim, return_sequences=True)(x)\n",
    "    for lay in range(int(lays_seq)):\n",
    "        x = add_deep_layers(x, dropout, dim)\n",
    "    \n",
    "    x = Bidirectional(LSTM(dim, return_sequences=False))(x)\n",
    "    #x = LSTM(dim, return_sequences=False)(x)\n",
    "    \n",
    "    for lay in range(int(lays)):\n",
    "        x = add_deep_layers(x, dropout, dim)\n",
    "        \n",
    "    x = add_deep_layers(x, dropout, 50)\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    x = Dense(2, activation='softmax',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "\n",
    "    return Model(inputs=[inputX], outputs=x)\n",
    "\n",
    "path = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mc7U6kNeMg_s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (212129, 6) (212129, 2) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "HP_NUM_SEQS = hp.HParam('num_seqs', hp.Discrete([50, 100, 150]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([150, 600]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.5]))\n",
    "HP_LAY_SEQ = hp.HParam('layers_seq', hp.Discrete([5]))\n",
    "HP_LAY = hp.HParam('layers', hp.Discrete([5, 8]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "METRIC_LOSS = 'loss'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_SEQS, HP_NUM_UNITS, HP_DROPOUT, HP_LAY_SEQ, HP_LAY ],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "  \n",
    "trainX, trainY, positiveX, positiveY, negativeX, negativeY = load_data(\"omgusd\", \"train\", path, balanced = False)\n",
    "print(\"Loaded: {} {} \".format(trainX.shape, trainY.shape))\n",
    "features = trainX.shape[-1]\n",
    "\n",
    "def train_test_model(hparams):\n",
    "    reset_seed()\n",
    "    \n",
    "    dim = hparams[HP_NUM_UNITS]\n",
    "    seqs = hparams[HP_NUM_SEQS]\n",
    "    dropout = hparams[HP_DROPOUT]\n",
    "    lays = hparams[HP_LAY]\n",
    "    lays_seq = hparams[HP_LAY_SEQ]\n",
    "\n",
    "    x, y = create_dataset(trainX, trainY, seqs)\n",
    "    \n",
    "    model = generate_model(seqs = seqs,\n",
    "                           features = features,\n",
    "                           dim = dim,\n",
    "                           dropout = dropout,\n",
    "                           lays = lays,\n",
    "                           lays_seq = lays_seq\n",
    "                          )\n",
    "    \n",
    "    #radam = tfa.optimizers.RectifiedAdam()\n",
    "    #ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "    model.compile(\n",
    "        #optimizer=ranger,\n",
    "        optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07),\n",
    "        loss=tf.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x,\n",
    "        y,\n",
    "        batch_size = 64*2,\n",
    "        shuffle=True,\n",
    "        #steps_per_epoch = len(train_generator),\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_loaded = model\n",
    "    \n",
    "    def average(lst): \n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "    def get_acc(db):\n",
    "        print(\"evaluating {}\".format(db))\n",
    "        valX, valY, positiveX_, positiveY_, negativeX_, negativeY_ = load_data(db, \"val\", path, balanced = True)\n",
    "        pos_x, pos_y = create_dataset(positiveX_, positiveY_, seqs)\n",
    "        neg_x, neg_y = create_dataset(negativeX_, negativeY_, seqs)\n",
    "        _, acc_pos = model_loaded.evaluate(pos_x, pos_y)\n",
    "        _, acc_neg = model_loaded.evaluate(neg_x, neg_y)\n",
    "        return average([acc_pos, acc_neg])\n",
    "    \n",
    "    evals = []\n",
    "    evals.append(get_acc(\"\"))\n",
    "    #evals.append(get_acc(\"btcusd\"))\n",
    "    #evals.append(get_acc(\"ethusd\"))\n",
    "    #evals.append(get_acc(\"ltcusd\"))\n",
    "    accuracy = average(evals)\n",
    "    print(\"accuracy: {}\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFmIqtn9Cc5q"
   },
   "outputs": [],
   "source": [
    "best_hparams = {}\n",
    "best_acc = 0\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "atSmRzAWClm2",
    "outputId": "b6da6fcf-4946-4dce-d7ed-e2805b0c1e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hparams: 12\n",
      "--- Starting trial: run-0\n",
      "{'num_seqs': 50, 'num_units': 150, 'dropout': 0.5, 'layers_seq': 5, 'layers': 5}\n",
      "Epoch 1/10\n",
      "1657/1657 [==============================] - 670s 401ms/step - loss: 0.6681 - accuracy: 0.9774\n",
      "Epoch 2/10\n",
      "1657/1657 [==============================] - 627s 378ms/step - loss: 0.5054 - accuracy: 0.9890\n",
      "Epoch 3/10\n",
      "1657/1657 [==============================] - 596s 359ms/step - loss: 0.1243 - accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "1657/1657 [==============================] - 595s 359ms/step - loss: 0.0697 - accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "1657/1657 [==============================] - 595s 359ms/step - loss: 0.0684 - accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "1657/1657 [==============================] - 597s 360ms/step - loss: 0.0677 - accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1657/1657 [==============================] - 596s 360ms/step - loss: 0.0675 - accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "1657/1657 [==============================] - 597s 360ms/step - loss: 0.0677 - accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "1657/1657 [==============================] - 601s 362ms/step - loss: 0.0671 - accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "1657/1657 [==============================] - 604s 364ms/step - loss: 0.0666 - accuracy: 0.9890\n",
      "evaluating \n",
      "24/24 [==============================] - 2s 34ms/step - loss: 4.1023 - accuracy: 0.0000e+00\n",
      "441/441 [==============================] - 16s 35ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "accuracy: 0.5\n",
      "--- Starting trial: run-1\n",
      "{'num_seqs': 50, 'num_units': 150, 'dropout': 0.5, 'layers_seq': 5, 'layers': 8}\n",
      "Epoch 1/10\n",
      "1657/1657 [==============================] - 618s 370ms/step - loss: 0.6771 - accuracy: 0.8948\n",
      "Epoch 2/10\n",
      "1657/1657 [==============================] - 617s 372ms/step - loss: 0.6018 - accuracy: 0.9890\n",
      "Epoch 3/10\n",
      "1657/1657 [==============================] - 616s 372ms/step - loss: 0.3395 - accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "1657/1657 [==============================] - 615s 371ms/step - loss: 0.0900 - accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "1657/1657 [==============================] - 613s 370ms/step - loss: 0.0665 - accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "1657/1657 [==============================] - 615s 371ms/step - loss: 0.0641 - accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1657/1657 [==============================] - 618s 373ms/step - loss: 0.0636 - accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "1657/1657 [==============================] - 619s 374ms/step - loss: 0.0636 - accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "1657/1657 [==============================] - 618s 373ms/step - loss: 0.0632 - accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "1657/1657 [==============================] - 617s 373ms/step - loss: 0.0635 - accuracy: 0.9890\n",
      "evaluating \n",
      "24/24 [==============================] - 2s 34ms/step - loss: 4.7495 - accuracy: 0.0000e+00\n",
      "441/441 [==============================] - 16s 35ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "accuracy: 0.5\n",
      "--- Starting trial: run-2\n",
      "{'num_seqs': 50, 'num_units': 600, 'dropout': 0.5, 'layers_seq': 5, 'layers': 5}\n",
      "Epoch 1/10\n",
      " 471/1657 [=======>......................] - ETA: 57:15 - loss: 0.6781 - accuracy: 0.9591"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "hparams_list = []\n",
    "\n",
    "for num_seqs in HP_NUM_SEQS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for dropout_rate in HP_DROPOUT.domain.values:\n",
    "            for lay_seq in HP_LAY_SEQ.domain.values:\n",
    "                for lay in HP_LAY.domain.values:\n",
    "                    hparams = {\n",
    "                    HP_NUM_SEQS: num_seqs,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_LAY_SEQ: lay_seq,\n",
    "                    HP_LAY: lay\n",
    "                    }\n",
    "                    hparams_list.append(hparams)\n",
    "\n",
    "print(\"Total hparams: {}\".format(len(hparams_list)))\n",
    "\n",
    "for hparams in hparams_list:\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run_name = 'logs/hparam_tuning/' + run_name\n",
    "    run(run_name, hparams)\n",
    "    session_num += 1\n",
    "    \n",
    "print(\"Best acc {} hparams {}\".format(best_acc, best_hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Sy-tURRCuBT"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiP3mVyjILNn"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "WpKkcu78jHtY",
    "outputId": "c46220cf-515a-4a63-adf9-fdb550a5996a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             load_weights_on_restart=True)\n",
    "\n",
    "callbacks_list = []\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=val_generator, \n",
    "                              validation_steps=len(val_generator)-1,\n",
    "                              steps_per_epoch=len(train_generator)-1, \n",
    "                              epochs=20, verbose=1, \n",
    "                              callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py04DHTvEqyD"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkEcOQ1yHRdu"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wdVwTPStI27s",
    "outputId": "5ff2c4a2-3f33-4ab4-82f3-5f80b9727701"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print (datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRBnrMlx-hRI"
   },
   "outputs": [],
   "source": [
    "    inputX = Input(shape=(seqs, features))\n",
    "    x = Dense(dim)(inputX)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LSTM(units=dim, return_sequences=True)(x)\n",
    "    for lay in range(int(lays_seq)):\n",
    "        x = add_deep_layers(x, dropout, dim)\n",
    "    x = LSTM(dim, return_sequences=False)(x)\n",
    "    \n",
    "    for lay in range(int(lays)):\n",
    "        x = add_deep_layers(x, dropout, dim)\n",
    "        \n",
    "    x = add_deep_layers(x, dropout, 20)\n",
    "    x = Dense(10)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stock_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
