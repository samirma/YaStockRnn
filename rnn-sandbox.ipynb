{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 503083,
     "status": "ok",
     "timestamp": 1568068274112,
     "user": {
      "displayName": "Samir Moreira Antônio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCsh3ldsSLN0VnFRYp341EzO_UyuRNKmLncUbMA3Q=s64",
      "userId": "14581632111443153742"
     },
     "user_tz": -60
    },
    "id": "fGVQczSlF-9o",
    "outputId": "e3a06798-f737-48d9-ec6e-d3784cec3115"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from data_generator import DataGenerator\n",
    "from state_util import StateUtil\n",
    "from tec_an import TecAn\n",
    "import numpy as np\n",
    "from data_util import *\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scs\n",
    "\n",
    "from tensorflow.keras.layers import InputLayer, BatchNormalization, GlobalMaxPool1D, Bidirectional, Dense, Flatten, Conv2D, LeakyReLU, Dropout, LSTM, GRU, Input\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_util\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from data_generator import DataGenerator\n",
    "from state_util import StateUtil\n",
    "from tec_an import TecAn\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "trainX_raw, trainY_raw = load_data(\"\", \"train\", path)\n",
    "valX_raw, valY_raw = load_data(\"\", \"Val\", path)\n",
    "\n",
    "#trainX_balanced, trainY_balanced = get_balanced_set(trainX_raw, trainY_raw)\n",
    "\n",
    "valX, valY = valX_raw, valY_raw\n",
    "\n",
    "features = trainX_raw.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26510, 20, 26) (26510,) | (2434, 20, 26) (2434,)\n"
     ]
    }
   ],
   "source": [
    "#print(Counter(trainY))\n",
    "\n",
    "time_steps = 20\n",
    "\n",
    "train_x_seq, train_y_seq = create_dataset(trainX_raw, trainY_raw , time_steps=time_steps, null_value = 0)\n",
    "val_x_seq, val_y_seq = create_dataset(valX, valY, time_steps=time_steps, null_value = 0)\n",
    "\n",
    "#print(Counter(train_y_seq))\n",
    "\n",
    "X_over, y_over = get_balanced_set_seq(train_x_seq, train_y_seq)\n",
    "\n",
    "#print(Counter(y_over))\n",
    "\n",
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(input_shape=(time_steps, features))\n",
    "normalizer.adapt(X_over)\n",
    "\n",
    "print(\"{} {} | {} {}\".format(X_over.shape, y_over.shape, val_x_seq.shape, val_y_seq.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "def get_balanced_set(x, y):\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    #print(Counter(y))\n",
    "    X_over, y_over = oversample.fit_resample(x, y)\n",
    "    #print(Counter(y_over))\n",
    "    return X_over, y_over\n",
    "\n",
    "\n",
    "metric = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90 µs, sys: 15 µs, total: 105 µs\n",
      "Wall time: 119 µs\n",
      "CPU times: user 8.08 ms, sys: 1.54 ms, total: 9.62 ms\n",
      "Wall time: 8.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def get_full_model(\n",
    "                  seqs = time_steps, \n",
    "                  features = 3, \n",
    "                  layers = 1, \n",
    "                  units = 5, \n",
    "                  optimizer='Adam',\n",
    "                  activation='relu',\n",
    "                  k_reg=regularizers.l2(0.01),\n",
    "                  init='glorot_uniform',\n",
    "                  lays_seqs = 3\n",
    "                 ):\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #, input_shape=(seqs, features)\n",
    "    \n",
    "    model.add(normalizer)\n",
    "        \n",
    "    model.add(Dense(units, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation=activation))\n",
    "    \n",
    "\n",
    "    for lay in range(int(layers)):\n",
    "        model.add(Dense(units, \n",
    "                        kernel_initializer=init, \n",
    "                        kernel_regularizer=k_reg,\n",
    "                        activation=activation))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation='sigmoid')) \n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_rnn_model(\n",
    "                  seqs = time_steps, \n",
    "                  features = 3, \n",
    "                  layers = 1, \n",
    "                  units = 5, \n",
    "                  optimizer='Adam',\n",
    "                  activation='relu',\n",
    "                  k_reg=regularizers.l2(0.01),\n",
    "                  init='glorot_uniform',\n",
    "                  lays_seqs = 3\n",
    "                 ):\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #, input_shape=(seqs, features)\n",
    "    \n",
    "    model.add(normalizer)\n",
    "        \n",
    "    model.add(Dense(units, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation=activation))\n",
    "    \n",
    "    for lay in range(int(lays_seqs)):\n",
    "        model.add(LSTM(units, kernel_initializer=init, return_sequences=True))\n",
    "        #model.add(GRU(units, kernel_initializer=init, return_sequences=True))\n",
    "    \n",
    "    #x = Bidirectional(LSTM(dim, return_sequences=False))(x)\n",
    "    model.add(LSTM(units, kernel_initializer=init, return_sequences=False))\n",
    "    #model.add(GRU(units, kernel_initializer=init, return_sequences=False))\n",
    "\n",
    "\n",
    "    for lay in range(int(layers)):\n",
    "        model.add(Dense(units, \n",
    "                        kernel_initializer=init, \n",
    "                        kernel_regularizer=k_reg,\n",
    "                        activation=activation))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, \n",
    "                  kernel_initializer=init,\n",
    "                  kernel_regularizer=k_reg,\n",
    "                  activation='sigmoid')) \n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#model = get_rnn_model(**params)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=12.4min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=11.8min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=12.7min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=11.5min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=17.4min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=17.7min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=18.5min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=18.4min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 9.8min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 9.9min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=10.2min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=10.2min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=17.8min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=17.6min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=18.3min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=18.4min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 9.7min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 9.6min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 9.8min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.3min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=14.7min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=14.8min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=15.2min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=15.3min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 8.1min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 8.1min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 8.4min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.4min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=14.9min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=14.8min\n",
      "[CV 1/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=15.4min\n",
      "[CV 2/2] END activation=relu, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=15.3min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 8.1min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 7.9min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 8.3min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.3min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=14.7min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=14.7min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=15.2min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=15.2min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 8.2min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 8.1min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 8.4min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.5min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=15.1min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=14.7min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=15.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l2, layers=20, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=15.3min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 8.1min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 8.0min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 8.2min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.3min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time=14.7min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time=14.7min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time=15.1min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=15, lays_seqs=10, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time=15.2min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.499 total time= 8.1min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Adadelta, seqs=20, units=162;, score=0.501 total time= 8.1min\n",
      "[CV 1/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.499 total time= 8.3min\n",
      "[CV 2/2] END activation=tanh, batch_size=64, epochs=100, features=26, k_reg=l1, layers=20, lays_seqs=5, optimizer=Nadam, seqs=20, units=162;, score=0.501 total time= 8.4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get_full_model\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=get_rnn_model, \n",
    "                        validation_data = (val_x_seq, val_y_seq),\n",
    "                        verbose=0\n",
    "                       )\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = [\n",
    "              \"Adadelta\",\n",
    "              \"Nadam\"\n",
    "              #, \"Adam\"\n",
    "]\n",
    "\n",
    "init = [\n",
    "        'glorot_uniform', \n",
    "        #, 'normal' \n",
    "        'he_uniform'\n",
    "        ]\n",
    "\n",
    "activation = [\n",
    "        #'softmax', \n",
    "        #'softplus', \n",
    "        #'softsign', \n",
    "        'relu', \n",
    "        'tanh', \n",
    "        #'sigmoid', \n",
    "        #'hard_sigmoid', \n",
    "        #'linear'\n",
    "        ]\n",
    "\n",
    "epochs = [100]\n",
    "batches = [64]\n",
    "features = [features]\n",
    "seqs = [time_steps]\n",
    "layers = [15, 20]\n",
    "units = [162]\n",
    "regs = [ \"l2\", \"l1\"]\n",
    "lays_seqs = [5, 10]\n",
    "\n",
    "#get_simple_model(features[0]).summary()\n",
    "\n",
    "ea = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "param_grid = dict(\n",
    "                  seqs = seqs,\n",
    "                  optimizer=optimizers, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batches,\n",
    "                  layers = layers,\n",
    "                  units = units,\n",
    "                  features = features,\n",
    "                  lays_seqs = lays_seqs,\n",
    "                  k_reg=regs,\n",
    "                  activation = activation\n",
    "                  )\n",
    "\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid,\n",
    "                    verbose=4,\n",
    "                    cv=2,\n",
    "                    #scoring = 'average_precision'\n",
    "                    #n_jobs = -1\n",
    "                   )\n",
    "\n",
    "grid_result = grid.fit(X_over, y_over, callbacks=[ea])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "params = grid_result.best_params_\n",
    "del params['batch_size']\n",
    "del params['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = get_rnn_model(**params)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "          X_over, y_over,\n",
    "          epochs=100, \n",
    "          verbose=False,\n",
    "          batch_size=64,\n",
    "          callbacks=[ea],\n",
    "          validation_data = (val_x_seq, val_y_seq),\n",
    "          )\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.history.history)\n",
    "res[[metric, 'val_' + metric]].plot(figsize=(10, 6), style='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def eval_data(model, x, y):\n",
    "\n",
    "    print(model.evaluate(x, y))\n",
    "\n",
    "    pred = np.where(model.predict(x) > 0.5,1,0)\n",
    "\n",
    "    print(\"Confusion Matrix {} {}\".format(pred.shape, y.shape))\n",
    "    print(confusion_matrix(y, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "#print(eval_data(model, training_data[cols], training_data['direction']))\n",
    "print(eval_data(model, X_over, y_over))\n",
    "print(\"Test\")\n",
    "#print(eval_data(model, test_data[cols], test_data['direction']))\n",
    "print(eval_data(model, val_x_seq, val_y_seq))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "enviroment",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
